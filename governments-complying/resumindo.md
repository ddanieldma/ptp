Please help me summarizing the key points of the following paper: 

Comparing the operation of government policies across different
jurisdictions not only helps identify better practices and gaps in implementation or compliance, but powerfully motivates political commitments (Kelley & Simmons, 2020). On this last count, transparency and freedom of information policies are no exception; scholarly research on freedom of information (FOI) has shown that comparative legal evaluations can positively impact the strength of laws (Berliner, 2015).
However, we still lack comparative bodies of evidence on compliance to assess the performance of governmental transparency laws around the world, particularly among different branches and levels of government.
In this paper, we lay out on an ongoing research agenda to address this deficit. We explore data from an exhaustive compilation of 265 transparency audit-type evaluations from across Latin America, authored during nearly 15 years (2003 to the end of 2017) by governmental oversight agencies, academics, and NGOs – since the region’s first FOI laws took effect. Data from these 265 evaluations, which gauge the compliance of governments with passive transparency (governmental responses to citizen requests for information) and active transparency (website-based governmental disclosure), illuminate comparative trends on how public administrations in different countries, branches, and levels of government abide by their own relatively new transparency statutes. They also bring to light what policy themes (e.g. health, education) receive most attention and how the focus and results of evaluations vary depending on who does the evaluating (what type of stakeholder-evaluator). While the diversity of evaluations in our compilation understandably renders them ‘imperfectly comparable’, data on compliance furnishes a common denominator. Perhaps more importantly, the diversity of authors, methods, and policy themes represented in our database helps triangulate potential biases.
In this sense, the evaluative strategy we adopt, based on compilation and aggregation, innovates and fills a gap in the scholarly and grey literature on the comparative compliance of transparency regimes. Most comparative scholarly analyses of governmental transparency performance fall into one of three general approaches. A first set of works focuses on de jure laws, including their comparative legal merits (Access Info and the Centre for Law and Democracy, 2011; Banisar, 2011; Mendel, 2003, 2009) or their determinants (Berliner, 2014; Berliner & Erlich, 2013; Ingrams, 2018; Michener, 2015a, 2015b). A second group of works looks at the ‘implementation’ and ‘operation’ of transparency regimes, typically analyzing a small number of cases, usually at the national level, and including de jure analyses of transparency – including laws, regulations, reforms, and precedents – alongside more de facto performance-related measures, such as oversight actions, government statistics, compliance audits, and operational infrastructure (e.g. online platforms, information officers), among other factors (Adu, 2018; Calland & Bentley, 2013; Camaj, 2016; Darch & Underwood, 2010; Dragos¸, Neamtt¸u, & CobaRzan, ˆ 2012; Hazell & Worthy, 2010; Michener, 2015b; Michener, Niskier, & Contreras, 2018; Piotrowski, Zhang, Lin, & Yu, 2009; Relly, Rabbi, Sabharwal, Pakanati, & Schwalbe, 2020; Ríos Cazares & Cejudo, 2013; Roberts, 2006, 2010). A last set of works focuses on comparative compliance, but tends to do so within one country either as a means of conducting field experiments (Ben-Aaron, Denny, Desmarais, & Wallach, 2017; Cuillier, 2010; Michener, Velasco Braem, Contreras, & Furtado Rodrigues, 2019; Spa´ˇc, Voda, & Zagrapan, 2018) or shedding light on the determinants of subnational transparency compliance (Alcantara, Leone, & Spicer, 2016; Araujo & TejedoRomero, 2016; Armstrong, 2011; Bearfield & Bowman, 2017; Berliner, 2016; de Araujo & Tejedo-Romero, 2016; Dragos¸ et al., 2012; GarcíaTabuyo, S´ aez-Martín, & Caba-P´erez, 2016; Guillamon, ´ Bastida, & Benito, 2011; Lowatcharin & Menifield, 2015; Pina ˜ & Avellaneda, 2019; S´ aez-Martín, Lopez-Hern ´ andez, ´ & Caba-P´erez, 2019; Siˇcakov ´ a-Beblav ´ a, ´ Kollarik, ´ & Sloboda, 2016; Silva, Bruni, Silva, & Bruni, 2019; Sp´ aˇc et al., 2018; Tavares & da Cruz, 2020).
While the above approaches to gauging transparency have advanced our knowledge on the determinants of implementation and compliance, they provide relatively localized comparisons, typically focused on the executive branch – thus excluding other branches and levels of government – and usually examine only one type of transparency – either active transparency (website-based disclosure) or passive transparency (request-based disclosure). Moreover, the metrics most of these studies draw upon (i.e. indexes) often combine de jure and de facto measures of transparency. In the end, this composite approach may say little about compliance; possessing a strong law or the personnel or infrastructure in place for feeding websites and fielding FOI requests may not translate into compliance. At other times, governments with little operational preparedness may elicit vigorous compliance. In sum, many of the conclusions about the performance of FOI regimes advanced in the literature stand on thin ground; scholarly works have yet to focus on the issue of comparative compliance in any systematic manner. The primary obstacle, of course, is a paucity of data.
In addressing this deficit, the current study begins to shed light on at least four important patterns of regional cross-national compliance and transparency evaluation: a) the direction of transparency compliance trends across an entire region (Latin America), b) the type of transparency evaluated (i.e. passive v. active), c) the institutional focus of evaluations (i.e. levels and branches of government, policy themes), and, d) the evaluative and scoring propensities of different types of evaluators (e.g. government oversight agencies v. NGOs). Before delving into these patterns, the first section looks more closely at the data dilemmas of gauging comparative transparency compliance, dilemmas that motivated this study. The second section then details the methods behind our compilation and aggregation strategy. Following an exploration of our findings, the third section reflects on the limitations of using aggregated evaluations to understand the operation of transparency policies and, based on these reflections and our findings, provides recommendations for scholars and policymakers. 

2. Dilemmas of gauging comparative transparency compliance
Audit-type evaluations of policy compliance represent indispensableindicators of how well policies are working. Unlike perceptions or interviews, audit-like evaluations provide first-hand experiential measures of policy efficacy. Unlike government statistics, they are lesssusceptible to embellishment and obfuscation and are more comparable(Darch, 2013; Michener et al., 2018, pp. 619–620). Cross-jurisdictionalcomparability is another important issue. While many works haveleaned heavily on government statistics to compare FOI regimes (Hazell& Worthy, 2010; Michener, 2015b; Roberts, 2006), record-keepingpractices can vary wildly among jurisdictions and, perhaps moreimportantly, requests and responses in federal countries (e.g. Mexico)cannot be easily compared to those in unitarian countries (e.g. Chile).States and municipalities in federal systems possess greater constitutional rights and responsibilities and therefore tend to attract a largershare of total requests than in unitary systems (where central governments are the locus of authority). 
One might consider large-scale, ongoing and internally consistent transparency audit-type evaluations to be one of the better barometers of comparative transparency compliance. Yet not only are such evaluations expensive and difficult to coordinate, but also politically contentious, susceptible to charges of bias from the perspective of what is measured, how and by whom. It should not be surprising, then, that only one such transparency evaluation has ever taken place (Open Society - Justice Initiative, 2006). In the absence of ongoing, large-scale evaluative interventions, the world is increasingly awash in small, standalone transparency evaluations. Here too, the problems are familiar; n not only might evaluative initiatives be accused of political or methodological bias, but most evaluations contain so few observations per evaluated-agency – FOI requests (passive transparency) or websites assessments (active transparency) – that they can hardly be relied upon as representative barometers of a government or institution’s policy commitments.
The approach we adopt here is to leverage the power of aggregation and triangulation. By aggregating the findings from several hundred transparency evaluations undertaken by diverse stakeholders, we are able to draw inferences regarding compliance while ‘triangulating’ different types of bias. In other words, data derived from the compilation of a large number of evaluations is to some extent balanced by the biases of different types of evaluators (e.g. government v. NGO), who use different methodological approaches. Meanwhile, aggregation tends to flatten out some representativeness biases arising from varying levels of compliance in different branches, levels of government, and diverse policy themes – from procurement, to education, the environment, and so forth. Finally, the current approach to gauging compliance honors transparency evaluators and signals the importance of efforts aimed at advancing governmental transparency.
Latin America represents a valuable case for exploring governmental compliance with public transparency policies. This region’s countries – almost all of which enacted FOI laws between 2002 and 2016 – are broadly representative of the nearly 80 countries to have enacted FOI laws over the last two decades. Latin America represents lower to middle-income countries with moderate to severe governance challenges (Zovatto, 2016), similar to most recent FOI adopters. Furthermore, Latin American FOI laws score (91 out of 160 points) about equal to the world average on the Global RTI-Rating (89 out of 160 points), which is also the situation of countries that have adopted laws since the year 2000 (average 92.8/160 points). The combination of strong FOI laws in countries with questionable governance track records – many of them relatively recent democracies – gives us reason to question whether commitments are being translated into action. The next section examines how we extracted measures of governmental compliance from transparency evaluations, compliance being perhaps the strongest indicator of governmental ‘commitments’. 

3. Methodology and data
3.1. Definitions
Our database is compiled from active and passive transparency evaluations across Latin America during a period totaling 15 years, beginning with the year 2003 – one year after the region’s first FOI laws were enacted – and stopping at the end of 2017. By Latin America, we mean all countries in the Americas with Iberian (Spanish, Portuguese) heritage. In terms of active and passive transparency, we included in our database only evaluations that assessed how government responds to requests for information (passive transparency) or assessed obligatory or voluntary governmental disclosure of information in the public domain, typically on their websites (active transparency). We only included evaluations that examined governmental conformity with national or international legal statutes.
What we refer to as an ‘evaluation’ here includes only publicly available reports associated with a recognized organization or institution, which, in whole or in part, measure governmental compliance with public transparency policies. This definition therefore excludes evaluations that look solely at de jure laws, indicators of implementation (e.g. government statistics on requests, the presence of online requesting portals), or non-experiential measures, such as perceptions. By ‘report’ we refer to a “thorough accounting” with a clearly identified methodology. As for the inclusion of government evaluations in our database, we observed one rule: no self-evaluations were allowed. No institution could be both the author and the evaluated subject. In Honduras, for example, evaluations undertaken by government come from the Access to Public Information Institute, rather than individual agencies. 

3.2. Data collection
Strategies for data collection were centered on both evaluators and evaluations. Reporting back to a larger team, two researchers independently collected and compiled evaluations beginning in June 2016 and ending in April 2020. The identification of evaluations occurred via manual and machine-based strategies. Manually, evaluators perused both international and regional hubs (e.g. Regional Alliance for Freedom of Expression and Information) as well as country-specific organizations (i.e. NGOs). To identify academic works on FOI compliance, a thorough check of Google Scholar and searches of major university libraries resulted in several evaluations. For each country, all collection efforts involved thorough searches of national Google websites (e.g., Google Mexico). To ensure the comprehensiveness of our search, we deployed a webcrawler to search for PDF or HTML evaluations in Google, Google Scholar, and Bing in English, Spanish, and Portuguese. From our preexisting and growing database of evaluations, the crawler ‘learned’ what a transparency evaluation looked like. Researchers sifted through the results to validate or discard content provided by the crawler. A full list of our evaluations can be found on our online repository.
Evaluation authors fall into three major types, including a) academic research institutions, b) government oversight agencies (e.g. information commissioners or central bodies responsible for oversight), and c) NGOs, which encompass civil society organizations and multilateral organizations. These are non-exclusive types, as it is possible to find evaluations authored in combined efforts, such as academic-NGO collaborations.

3.3. Data analysis and entry
The analysis of evaluations and variables was undertaken by one trained coder and vetted by a second researcher following the data-entry process. The coder would read reports and conduct keyword searches to verify that all data regarding compliance had been located. Following a strict protocol, the coder would then take reported compliance rates directly from evaluations, checking for face validity. For active transparency, compliance rates are based on an assessment of public information disclosed by government agencies online, resulting in a score for each agency’s website evaluated. We calculated one unit of analysis as equal to one institution evaluated, meaning that even if scores were disaggregated by dimension (indicator) for each institution, we would take the institution’s aggregate score. For passive transparency, which we define as disclosure based on citizen requests and government responses, one unit of analysis is the response to an individual request.
Different evaluations diverge on what they consider ‘compliant’. For example, some evaluations report as compliant a government’s response to a FOI request that provides any answer: ‘no, we do not have the information’/ partial or complete disclosure / ‘we cannot find the information’ / ‘the information does not exist’, etc. Others, by contrast, report as compliant only requests that were answered with precise or complete information. We extracted response rates according to the following protocol: First, if the report’s authors clearly emphasized a response rate, we used that rate in our calculations. Self-reporting tended to be the case with most evaluations. If a response rate was not given but instead authors simply disaggregated responses by types of compliance – such as partial responses, denials, and claims of inexistent information – we interpreted anything as a response (compliance) that did not constitute a mute denial or a notice regarding internal processing or transfers. As much as these methodological issues are controverted and merit debate among transparency researchers and policy advocates, the diversity of approaches does represent a form of triangulation. In other words, divergent interpretations regarding what constitutes a ‘response’ ultimately provide us with results that ‘average out’ the biases of different approaches.
In presenting findings, we offer both simple (i.e. average compliance of the sum of evaluations) and weighted calculations (i.e. average compliance based on sum of observations). For example, if one Guatemalan active transparency evaluation assessed five agencies and the average compliance rate was 50%; and another evaluated fifty with a 60% average compliance rate, the simple compliance rate would be 55% while the weighted compliance rate would amount to 59%.

4. Findings
In all, extensive searches resulted in 418 transparency evaluations across Latin America. However, many evaluations provided qualitative information only. For the purposes of this paper, only those that provided numbers on compliance were included in our sample, amounting to an N of 265. Out of these evaluations, a total of 50 or 18.8% were applied in countries that either lack FOI laws or had no law at the time of evaluation. The list of all evaluations used for this work, all bibliographic details, and a full dataset can be found at an online repository.3 Table 1 provides compliance rates for all evaluations, by type of evaluator, for both simple averages and weighted averages and for active and passive transparency. The second shows the number of evaluations for active and passive transparency authored by an evaluator and by type of transparency. As described above, simple averages of compliance count all evaluations as equal, whereas weighted averages of compliance take into account the number of observations (website evaluations/requests). The sum of total evaluations is more than 265 because several evaluations measure both active and passive transparency.
Table 1 makes clear that the largest share of evaluations come from NGOs. It should also make clear that compliance rates for government are significantly higher than for other evaluators and that most evaluations focus on active transparency, particularly those authored by government. We will explore these patterns in greater detail later. For now, it is important to provide further context regarding our data. Table 2 illustrates the distribution of evaluations by country. Several evaluations are cross-national, covering more than one country, and the sum of the numbers presented is therefore greater than the total number of evaluations included in our database.
Chile, Peru and Panama are the only countries in Latin America in which NGOs are not the main origin of evaluations. The data also show unexpected results, with Chile – a relatively small country – boasting the largest number of evaluations, most of which are authored by the government’s information commissioner, the Transparency Council (Consejopara la Transparencia). It is somewhat surprising that Brazil has the greatest number of evaluations by nonprofits (NGOs) and academics, given that this country is a relatively late FOI adopter (2011) by regional standards.

4.1. Evaluative diversity A defining characteristic of compiled transparency evaluations is their striking diversity. Many evaluations contain little more than a dozen pages or so, whereas others resemble sophisticated books and merit international distribution. The former includes yearly monitoring exercises by NGOs in smaller Latin American countries. The Dominican Republic’s chapter of Transparency International, Participaci´ on Ciudadana, for example, carries out a yearly monitoring exercise of the country’s FOI law, a straightforward document that is typically less than 20 pages long. By contrast, Mexico’s Centro de Investigacion ´ y Docencia Economicas ´ (CIDE) and Brazil’s Fundaçao ˜ Getulio Vargas, two leading universities in the region, produce lengthy, rigorous reports. One CIDE (2014) evaluation involved researchers conducting 45-min interviews with 131 information officers and senior bureaucrats, sending out 2950 requests and making 1872 unique visits to 624 state-level web portals. Presented just before the approval of Mexico’s General FOI law in 2015, the evaluation helped shape discussions that would influence the content of this precedent-setting law (Rios-Cazares 2018). The collected body of evaluations also includes large numbers of government auditing exercises, some of which are semi-annual (Peru), annual (Brazil) or occur multiple times a year (Chile).

4.2. Text analysis of policy themes evaluated To schematically understand the policy themes covered by evaluations, we utilized automated text analysis. The first step was to convert all 265 evaluations into plain text. Our approach focused primarily on institutions, as most references to policy themes occurred in the context of relevant institutions. The number and variety of institutions responsible for a single policy theme, such as health or education, is large and diverse. Hence our second task was to create a list of terms related to public institutions and their related policy themes. Using the Spacy software package to identify terms in Spanish and Portuguese, we found 80,193 terms containing all possible strings for public institutions and their related policy themes. We then proceeded to manually filter the terms for public institutions, reducing the dataset to a list of 3001 unique terms. Each term was then associated with the evaluations in which they appeared, grouped by country.
Lastly, we selected the top-ten most frequent terms in each country. Since these terms were related to public institutions, we manually correlated each institution with its respective policy theme. For example, ‘ministerio de educacion´ ’ is an institution related to education, while ‘senado’ belongs to the legislative branch. This resulted in a list of the most frequent policy themes (based on terms) for each country. Fig. 1 illustrates a ranking of the top-ten most evaluated policy themes. In several cases, such as that of Argentina, the frequency of public institutions resulted in a tie, which meant that Argentina appears in more than ten policy themes.

4.3. Results of policy theme text analysis
Education constitutes the most salient policy theme in our database, followed by financial and monetary policy, social benefits (social services and security), and accountability. The focus on education is also apparent based on a cursory analysis of evaluations collected. Twelve evaluations are explicitly concerned with education, ten of which focus specifically on universities. Perhaps somewhat surprisingly, evaluations on health and the environment appear in the top-ten in only five countries. The judiciary and legislative branches are much less evaluated than executive branch agencies, as we will see in upcoming sections, but nonetheless appear in the top ten most frequently mentioned institutions in five and six countries, respectively. 

4.4. Findings on transparency compliance 
Real-world compliance with legal obligations is a key measure of how well laws are working. The data show that transparency compliance rates from 2003 to 2017 average 66.2% for active transparency and 46.2% for passive transparency in Latin America. This difference between compliance with active and passive transparency is relatively unsurprising; passive transparency is often considered a tougher test of compliance because it imposes potentially infinite demands on officials, who are required to respond within finite timeframes. These values represent the weighted average for all observations. If we calculate unweighted, simple averages – averaging compliance rates from the sum of each evaluation, as opposed to all pooled observations – then compliance rates for active and passive transparency average 57% and 62.4%, respectively. Here, the differential is significantly smaller. Comparisons between the weighted and simple averages are useful for detecting evaluations that might skew the results due to a larger number of observations, a comparison further explored in Figs. 5 and 6. 

4.4.1. Active transparency
Compliance with active transparency increased from less than 40% to close to 60% in the span of a decade and a half (2003–18). Fig. 2 provides detailed averages for each country in four periods. The vertical axis represents rates of active transparency compliance (in percent). The horizontal axis represents four periods, with period 1 corresponding to evaluations undertaken before specific FOI legislation was enacted in that country, and periods 2, 3 and 4 corresponding to three equal periods between the end of 2017 and when a country’s law went into effect. Mexico, for example, whose law went into effect in 2003, would have the following periods represented by periods 2, 3 and 4: 2003–2007, 2008–2012, 2013–2017. In the case of Costa Rica and Venezuela, the only two countries that possess no FOI laws, cases were grouped only for period 1. Each dot in Fig. 2 represents a country’s average active transparency compliance rate for all evaluations during that period. The number of evaluations in each section is represented by the diameter of the circle. It focuses exclusively on active transparency because passive transparency has less continuous chronological data. We include the same plot for passive transparency in the online appendix.
The data show uneven patterns, with upward trends in compliance nearly universal, except for Brazil, El Salvador, and Mexico. Compliance rates in these countries appear to have worsened during the last period (period 4). In the case of Brazil, large evaluations of state and municipal entities during periods 3 and 4 explain this result. If only evaluations assessing national entities are considered, Brazil’s performance improves slightly over time. These distinctions suggest that monolithic comparative rankings based on our data are problematic; performance will vary depending on the number of evaluations focused on specific levels or branches of government. In other words, it is preferable to undertake comparisons among like institutions and like levels of government.
To this end, we compared average compliance rates for each branch and level of government combination for each country, relative to the regional average (refer to Table 3). To explain via example: the relative average compliance rate for active transparency in the executive branch at the municipal level in Chile is +17.02 percentage points (on a 0–100 scale) higher than the average rate for all other countries in the region for the same branch and level of government. The final column shows a simple average of how well the country performs relative to the regional average (excluding the performance of the country represented). 
The above results are obviously affected by the type of evaluator and vary significantly among different branches and levels of government. The next subsection begins to address these issues.

4.4.2. Active transparency scores by type of evaluator, branch and level of government
One might intuit that different evaluators have varying incentives to adopt stronger or weaker methodologies in their analyses. While government oversight authorities may shy away from tough evaluations that will make them ‘unpopular’ with subjected agencies and political masters, an NGO may design evaluations that push the boundaries of government transparency obligations. Figs. 3 and 4 help us to better understand relative patterns of evaluation and compliance by type of evaluator via boxplot distributions of evaluations by compliance rates, type of evaluator (color) and either level of government (Fig. 3) or branch (Fig. 4). Each dash represents one evaluation, and its position on the vertical axis corresponds to its compliance rates for active transparency. The grey line shows the average compliance rate for active transparency.
Data on active transparency compliance is considerably more voluminous than that of passive transparency, thus the focus of the analyses excludes passive transparency. As before, the equivalent figures for passive transparency can be found in the online appendix. Fig. 3 illustrates large variation in compliance rates at all levels of government, the dominance of national level evaluations (as opposed to evaluations on state or municipal governments), and higher scores awarded by governmental evaluators at all levels of government. Fig. 4, which depicts evaluations by branch of government, illustrates similar patterns. The executive branch receives the lion’s share of attention. Indeed, 62% of all active transparency evaluations are focused on the national executive branch. Given donor and multilateral priorities, a focus on the national level executive branch should not be particularly surprising. It should nonetheless give reason for reflection. After all, if we consider three levels of government and three branches (minus judicial institutions at the municipal level), we have eight state jurisdictions to evaluate. The national executive represents one-eighth of the state but receives nearly two-thirds of all evaluation efforts. In other words, Figs. 3 and 4 are notable for the lack of government evaluation occurring in the legislative and judicial branches and in all branches at the state and municipal levels of government. Most worrisome is the dearth of municipal evaluations, as municipal governance is critical for sustaining quotidian citizen needs such as security, sanitation, health, transport, and education.
The judiciary also receives comparatively little attention. Fig. 4 shows that most evaluations of the judiciary tend to be undertaken by NGOs, and approximately half of all evaluations occurring in the judiciary are undertaken by one organization, the Justice Studies Center for the Americas, which is funded by the Organization of American States.
As in Fig. 3, Fig. 4 shows that evaluations undertaken by government authorities score much higher than those carried out by academics or NGOs. With few exceptions, only government evaluators scored performance at levels higher than 80% compliance. Differences in compliance scores between government evaluations on the one hand, and all other evaluators, on the other, are statistically significant (MannWhitney u Test). Table 1 shows that average compliance rates with active transparency for all evaluators – except for government evaluators – fall within a 15% margin of each other – between 49.9% and 61.5% compliance. By contrast, governmental evaluations are more than 10 percentage points higher (72.7%).
Finally, in focusing on aggregate and average compliance rates, it is important to understand the influence of evaluations with a large number of observations. While weighted averages have the advantage of being empirically proportionate, they also diminish the relative weight of smaller evaluations. In order to address this issue, we compared compliance of the weighted average (all observations) to the simple average (all evaluations). Fig. 5 illustrates the distribution by country, evaluator, and levels of compliance with active transparency. Each circle represents one evaluation, with its diameter illustrating the number of websites analyzed by that evaluation. Two averages are represented – the weighted average (i.e. observations) for all evaluations (red vertical bar), and the simple average (i.e. evaluations) in that country (yellow vertical bar). This spread, between the averages of observations and evaluations (what we might call ’evaluator or evaluation coalescence’), should say something about the consistency of compliance scoring by different types of evaluators; the larger the spread, the greater the disagreement among evaluators on transparency scores. 
Most countries show relatively narrow spreads or high evaluation coalescence, with several exceptions, including El Salvador, Ecuador and, most jarringly, Honduras. Whereas governmental evaluations are skewing results to the right (higher compliance) in Ecuador and Honduras, in El Salvador it is the opposite, with NGO evaluations scoring compliance as higher than governmental evaluators. In El Salvador, nearly all of these evaluations come from one organization, Iniciativa Social para la Democracia. Similarly, several large academic evaluations undertaken by CIDE also skew results towards higher compliance. This last result is surprising, given the finding that evaluators outside of government generally tend to be more stringent in their scoring than other evaluators. They also tend to be more likely to evaluate passive transparency, as the results of the next section illustrate.

4.4.3. Passive transparency
Evaluators assessed passive transparency in 84 evaluations, which contain 39,305 requests for information. Findings on compliance with passive transparency obligations in Latin America share similarities with those for active transparency, including the extent of variation among evaluation scores, and the dominant focus on the national executive. However, unlike results for active transparency, differences in rates of compliance are much larger between levels of government. While requests to national level governments (executive branch) garnered an average of 68.1% compliance (12,091 observations), requests to municipal level governments obtained an average compliance score of 20.4% (19,358 observations). This 40% gap should raise concerns.
However, concerns must also take into account the origin of observations. Fig. 6 illustrates the difference between simple and weighted averages, wherein the response rate for all FOI requests averages 46.2%, nearly 15% lower than the 62.4% average for evaluations. Part of this difference can be explained by two sets of evaluations that contain large numbers of observations and skew the overall direction of results. The first one is Mexico’s Centro de Investigacion ´ y Docencia Economicas ´ (CIDE), whose evaluators produced several reports in the series, “Metríca de Transparencia”. These evaluations were responsible for thousands of requests in Mexico, with an incredibly high response rate of nearly 90%. It would be interesting to explore the extent to which this high response rate might be related to who was behind the requests (Michener et al., 2019). CIDE’s contribution to skewing the results is relatively minor compared to the effect of Brazil’s Office of the Comptroller General (CGU) and its annual ranking and reports, “Escala Brasil Transparente” (Brazil Transparency Ranking). Here, the CGU sent four requests per municipality to thousands of municipalities, garnering extremely poor response rates. Whereas CIDE sent a total of 4760 requests, the CGU sent nearly 18,000 requests. The effect of these two evaluations, however, does not change the fact that municipal compliance with passive transparency is weak compared to both a) rates at the national level and b) compliance with active transparency more generally.
In terms of the average evaluation coalescence or ‘spreads’ between passive transparency evaluations and observations (refer to Fig. 6), we see lower evaluation coalescence (higher spreads) in Mexico, Brazil and Argentina. In the case of Brazil and Mexico, the effects caused by CIDE and the CGU’s evaluations, as previously discussed, are naked to the eye. Chile’s performance on this measure is also worth noting. Given the number of observations and evaluations, Chile’s compliance rates are strikingly aligned, even though non-governmental scores are much lower than those of government evaluators.
An important observation derived from passive transparency evaluations is the comparative scarcity of governmental evaluations. The government is author of just 28.5% of all passive transparency evaluations, while for active transparency, this proportion is 39.5%. Among the 11 countries with passive transparency evaluations, only three countries had oversight agencies that undertook passive transparency evaluations (Brazil, Chile, and Mexico). It bears noting that these three countries are generally recognized as among the most advanced administrators of their national-level FOI regimes.

5. Conclusion
By compiling and aggregating compliance rates from 265 transparency evaluations, this article begins to fill a major gap on comparative rates of public sector compliance with transparency policies. Compliance rates are critical for gauging how well governments are honoring commitments to transparency in practice. Aggregate results from 15 years of evaluation (2003–2018) indicate modest gains in compliance – from less than 40% to around 60% for the decade and a half under analysis. Yet compliance is still significantly wanting. If all observations are pooled from all evaluations on active and passive transparency, compliance rates show that only two out of every three (66%) government websites are compliant and only one in two (46%) requests are compliant for passive transparency. If all evaluations are averaged (regardless of how many observations each contains), we end up with compliance rates of 57% for passive transparency and 62.4% for active transparency. Yet the most worrying compliance gaps are to be found in comparing municipal with national level governments. In the case of passive transparency, response rates are nearly 40% lower for municipal versus national level governments (20.4% versus 66.1%). In short, the numbers clearly show that compliance with transparency across the region is wanting. Nearly all research on implementation and compliance around the world confirms that Latin America is not alone in this respect.
While our efforts at comparing rates of public sector compliance with transparency provide indisputable evidence of ‘compliance gaps’, they also merit qualification on several levels. First, ‘compliance’ means different things depending on the evaluation and evaluator. This dilemma is most vividly illustrated by passive transparency evaluations. While some evaluations count FOI response rates as any response at all to a request, others calculate response rates as answers that fulfill prespecified criteria. Active transparency suffers from similar dilemmas. Whereas some evaluators – especially governments – use binary measures of the presence or absence of a category of government information (e.g. expenditures) to score compliance, other evaluators use ordinal scales to score the attributes of items within that category, such as the processability and granularity of expenditures. The task of forging consensus on definitions and measurements to gauge compliance with transparency remains urgent and should represent a priority for policymakers, activists, and scholars. A second reason why overall numbers on compliance merit qualification is the diversity of evaluative foci. A broad array of policy themes adds to the broad variation of compliance rates. For example, one might imagine that compliance rates in a policy theme such as public security operations will fare worse than one focused on budgetary information. Another source of variation in compliance rates are levels of government. Region-wide average scores for national-level active transparency compliance (69.7%) are considerably higher than state (58.8%) or municipal (52.3%) levels. While larger cities, especially capital cities, may perform well, smaller cities tend to do worse - a finding that is nearly universal in the literature on municipal transparency (see, for example, Sp´ aˇc et al., 2018).
Evaluative efforts might be an important part of the explanation for why municipal and other subnational governments post such inferior results. After all, disparities in the number of evaluations on different levels of government appear to correlate with varying levels of compliance. Perhaps the degree to which national executive branches are evaluated partly explains why they tend to perform better than other levels or branches of government. This relationship – more evaluation equals greater compliance – requires further research. The literature does point in this direction (Berliner, 2015; Hedlin, 2017); being evaluated can increase compliance independent of other factors, including pressure by external stakeholders.
A third qualification has to do with both the uneven distribution of evaluation on several different dimensions. A few evaluators in a few countries are responsible for most observations, skewing the overall direction of results. And most evalutors tend to evaluate passive transparency far less frequently than active transparency, which is especially the case among governmental evaluators. Only Brazil, Chile and Mexico featured passive transparency evaluations of government by governmental evaluators. The political implications of passive transparency evaluations likely explain these imbalances. By requesting information, passive transparency evaluations are clearly rigorous ‘tests’ of commitments to transparency and impose both symbolic and real burdens on administrators. Moreover, administrators receiving requests may identify requesters or requests and figure out that they are being ‘tested’. This recognition, in turn, can trigger Hawthorne effects (reactivity), posing threats to the validity of evaluation. An additional dilemma is the reliability of responses in terms of consistency. The same request to the same agency can prompt different responses, depending upon who responds or variegated aspects of the administrative context.
The last qualification to be made with regards to comparing compliance has to do with the value of decontextualized numbers. As scholars have noted (Darch, 2013, p. 112), “it is not hard to imagine ways in which ATI [FOI] rules might be complied with at a formal level, but the citizen’s needs and expectations are not met.” Acknowledging the validity of these claims, the current study was predicated on the belief that, while numbers on compliance may not be able to “quantify complex social phenomena” (Darch, 2013, p. 113), they do provide important general indicators. Furthermore, a key advantage of compiling and aggregating compliance data from many evaluations is greater pluralism, resulting in the triangulation of different methods, types of evaluators (e.g. NGO v. academics), and policy themes (e.g. health transparency) in calculating compliance.
In addition to providing qualifications regarding numbers on compliance, it is also important to note that although we conducted extensive searches, some transparency evaluations in Latin America will inevitably have gone unidentified. The hope is that this article may help galvanize a ‘Transparency Evaluation Network’, and that evaluators will contribute their neglected compliance evaluations to this ongoing initiative. This article represents the first large multi-stakeholder database of transparency evaluations ever assembled, a compilation that contains significant intrinsic and extrinsic value for policymakers, donors, scholars and activists. The hope is that this database will not be taken at face value but rather prompt discussion and debate. Only through dialogue can proponents of transparency establish solid grounding for universal definitions, measurements, increased evaluation, greater compliance and, ultimately, better informational outcomes for governments and citizens.